# Lecture 1: Introduction to SGD and Computational Graphs

## Overview 🌐

In this inaugural lecture, we embark on a foundational exploration of Stochastic Gradient Descent (SGD) and delve into the fundamentals of computational graphs. The aim is to establish a solid understanding of these essential concepts that underpin many machine learning algorithms.

## Resources 📚

- [SGD Introductory Notebook](./sgd-loss.ipynb)
- [Lecture Slides](./Autograd.pdf) (pptx version: [here](./Autograd.pptx))

## Key Topics Covered 🧠

1. **Stochastic Gradient Descent (SGD):**
   - Definition and significance in optimization.
   - Intuition behind the stochastic approach and its advantages.

2. **Computational Graphs:**
   - Explanation of computational graphs as representations of mathematical expressions.
   - Distinction between forward mode and reverse mode in computational graph differentiation.
   - Practical applications and relevance in machine learning.

## Learning Objectives 🎓

- Grasp the core principles of SGD and comprehend its role in optimizing models during the training phase.
- Understand how computational graphs serve as a powerful tool for computing complex derivatives of mathematical expressions.
- Differentiate between forward mode and reverse mode in the context of differentiation over computational graphs.
- Be able to draw computational graphs of simple mathematical expressions and compute reverse mode and forward mode derivatives by hand.

